{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spaceship Titanic\n",
        "\n",
        "The main goal of this competion is to predict whether a passenger was transported to an alternate dimension during the *Spaceship Titanic's* collision with the spacetime anomaly.\n",
        "\n",
        "* Link to the competition: https://www.kaggle.com/competitions/spaceship-titanic/overview"
      ],
      "metadata": {
        "id": "AupitrXXVcoN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data\n",
        "\n",
        "We have to different files:\n",
        "\n",
        "* **train.csv** - personal records for about two-thirds (~8700) of the passengers\n",
        "* **test.csv** - personal records for the remaining one-third (~4300) of the passengers. We will need to predict the value of `Transported` for the passengers in this set."
      ],
      "metadata": {
        "id": "XNHjvFRHVhO_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBplZIY0QEGM"
      },
      "outputs": [],
      "source": [
        "pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V1WQApuDkLW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve credentials\n",
        "KAGGLE_KEY =  userdata.get('KAGGLE_KEY')\n",
        "KAGGLE_USERNAME = userdata.get('KAGGLE_USERNAME')\n",
        "\n",
        "# Set environmental variables with %env to better work with kaggle\n",
        "%env KAGGLE_USERNAME=$KAGGLE_USERNAME\n",
        "%env KAGGLE_KEY=$KAGGLE_KEY"
      ],
      "metadata": {
        "id": "BmJGo7QWQ2BV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c spaceship-titanic\n"
      ],
      "metadata": {
        "id": "rzi15SkRRHfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/spaceship-titanic.zip"
      ],
      "metadata": {
        "id": "407xy4wjVCjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect Data"
      ],
      "metadata": {
        "id": "jzKwmFGrWkx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "train_df = pd.read_csv('/content/train.csv')"
      ],
      "metadata": {
        "id": "YtWp3G4gWmjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the train_df\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "y7M7A1-0XHjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "id": "Sb2dcFq9Xs97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "-ibyb3-JY0l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many data is missing in each column\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "loCaSoC1Y_6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization\n",
        "\n",
        "In this competition we want to see what passengers were `Transported` or not. We are going to see if we can find some type of relation between different values and if they were `Trasnported` or not transported."
      ],
      "metadata": {
        "id": "P1z5hkR7ZrvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "figs, axs = plt.subplots(2,2, figsize=(15, 10)) # Adjust figsize for better readability\n",
        "\n",
        "# Plot Transported vs VIP\n",
        "pd.crosstab(train_df[\"Transported\"], train_df[\"VIP\"]).plot(kind=\"bar\",color = [\"lightseagreen\", \"olivedrab\"], ax=axs[0][0])\n",
        "axs[0][0].set_title(\"Transported vs VIP\")\n",
        "axs[0][0].set_xlabel(\"Transported\")\n",
        "axs[0][0].set_ylabel(\"Count\")\n",
        "axs[0][0].tick_params(axis='x', rotation=0) # Rotate x-axis labels\n",
        "\n",
        "# Plot Transported vs CryoSleep\n",
        "pd.crosstab(train_df[\"Transported\"], train_df[\"CryoSleep\"]).plot(kind=\"bar\",color = [\"lightseagreen\", \"olivedrab\"], ax=axs[0][1])\n",
        "axs[0][1].set_title(\"Transported vs CryoSleep\")\n",
        "axs[0][1].set_xlabel(\"Transported\")\n",
        "axs[0][1].set_ylabel(\"Count\")\n",
        "axs[0][1].tick_params(axis='x', rotation=0) # Rotate x-axis labels\n",
        "\n",
        "# Plot Transported vs HomePlanet\n",
        "pd.crosstab(train_df[\"Transported\"], train_df[\"HomePlanet\"]).plot(kind=\"bar\",color = [\"lightseagreen\", \"olivedrab\", \"indigo\"], ax=axs[1][0])\n",
        "axs[1][0].set_title(\"Transported vs HomePlanet\")\n",
        "axs[1][0].set_xlabel(\"Transported\")\n",
        "axs[1][0].set_ylabel(\"Count\")\n",
        "axs[1][0].tick_params(axis='x', rotation=0) # Rotate x-axis labels\n",
        "\n",
        "\n",
        "# Plot Transported vs Age\n",
        "sns.kdeplot(data=train_df, x='Age', hue='Transported', fill=True, ax=axs[1][1], palette='magma')\n",
        "axs[1][1].set_title(\"Transported vs Age\")\n",
        "axs[1][1].legend(title='Transported Status', labels=['Not Transported', 'Transported'])\n",
        "\n",
        "\n",
        "plt.tight_layout() # Adjust layout to prevent overlapping titles/labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I_onO34P6xt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the different plots we can see that:\n",
        "\n",
        "* **VIPs** didn't have any special treatment to be selected or not selected.\n",
        "* Those who were in a **CryoSleep** were more selected than those who weren't\n",
        "* People from **Europa** and **Mars** were more selected that those from Earth\n",
        "* People from ages ~ 10 to ~35 were the ones more selected."
      ],
      "metadata": {
        "id": "KvYZrCMD8pFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "sBWBQps0IYtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to split `Cabin` and `GroupSize` columns into different columns spliting them by \"/\" or by \"_\".\n",
        "\n",
        " * `Cabin` is going to be split in `Deck`, `Num` and `Size`.\n",
        "\n",
        " * ` GroupSize` is going to be split in `Group_ID` and `Number_Group`. With this information we can create a new column called `Alone` where we are going to say True/False to the people who travel alone or in groups.\n",
        "\n",
        "After this we need to fill the missing data for the columns.\n",
        " * For the *numerical columns* we are going to use the median to fill those values\n",
        " * For the *categorical/boolean columns* we are going to use **.mode()** that is the value that appears most often to fill those missing values.\n",
        "\n",
        " After all this, we will drop the colmns that we don't need anymore."
      ],
      "metadata": {
        "id": "DpZ2hS62IuWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop Name\n",
        "train_df = train_df.drop('Name', axis=1)\n",
        "test_df = test_df.drop('Name', axis=1)"
      ],
      "metadata": {
        "id": "a13aXDshJt3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remind how many data is missing in each column\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "jAdUxyRkJ5t9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split Cabin in three diferent columns\n",
        "train_df[[\"Deck\", \"Num\", \"Side\"]] = train_df[\"Cabin\"].str.split(\"/\", expand = True)\n",
        "test_df[[\"Deck\", \"Num\", \"Side\"]] = test_df[\"Cabin\"].str.split(\"/\", expand = True)\n",
        "\n",
        "# Change Num column to int\n",
        "train_df[\"Num\"] = train_df[\"Num\"].astype(float)\n",
        "test_df[\"Num\"] = test_df[\"Num\"].astype(float)\n",
        "\n",
        "# Split Passaenger Id into two diffrerent columns\n",
        "train_df[[\"Group_ID\", \"Number_Group\"]] = train_df[\"PassengerId\"].str.split(\"_\", expand = True)\n",
        "test_df[[\"Group_ID\", \"Number_Group\"]] = test_df[\"PassengerId\"].str.split(\"_\", expand = True)"
      ],
      "metadata": {
        "id": "RoiVqYUTP8d1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate GroupSize\n",
        "train_df[\"GroupSize\"] = train_df.groupby(\"Group_ID\")[\"Group_ID\"].transform(\"count\")\n",
        "test_df[\"GroupSize\"] = test_df.groupby(\"Group_ID\")[\"Group_ID\"].transform(\"count\")\n",
        "\n",
        "# Drop PassangerID\n",
        "train_df = train_df.drop('PassengerId', axis=1)\n",
        "test_df = test_df.drop('PassengerId', axis=1)\n",
        "\n",
        "# Create a new column to see if the Passangers are alone or not\n",
        "train_df[\"Alone\"] = (train_df[\"GroupSize\"] == 1)\n",
        "test_df[\"Alone\"] = (test_df[\"GroupSize\"] == 1)\n",
        "\n",
        "# Change the Number group to an int\n",
        "train_df[\"Number_Group\"] = train_df[\"Number_Group\"].astype(int)\n",
        "test_df[\"Number_Group\"] = test_df[\"Number_Group\"].astype(int)\n",
        "\n",
        "# Drop Group_ID since we have the groupsize and the alone column\n",
        "train_df = train_df.drop('Group_ID', axis=1)\n",
        "test_df = test_df.drop('Group_ID', axis=1)"
      ],
      "metadata": {
        "id": "qVJUR4bRVWrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate median for numerical columns\n",
        "age_median = train_df[\"Age\"].median()\n",
        "room_service_median = train_df[\"RoomService\"].median()\n",
        "food_court_median = train_df[\"FoodCourt\"].median()\n",
        "shopping_mall_median = train_df[\"ShoppingMall\"].median()\n",
        "spa_median = train_df[\"Spa\"].median()\n",
        "vr_deck_median = train_df[\"VRDeck\"].median()\n",
        "num_median = train_df[\"Num\"].median()\n",
        "\n",
        "# Calculate mode (the value that appears the most often) for categorical/boolean columns\n",
        "home_planet_mode = train_df[\"HomePlanet\"].mode()[0]\n",
        "destination_mode = train_df[\"Destination\"].mode()[0]\n",
        "vip_mode = train_df[\"VIP\"].mode()[0]\n",
        "cryosleep_mode = train_df[\"CryoSleep\"].mode()[0]\n",
        "deck_mode = train_df[\"Deck\"].mode()[0]\n",
        "side_mode = train_df[\"Side\"].mode()[0]\n",
        "\n",
        "# Fill NA values in train_df\n",
        "train_df[\"Age\"].fillna(age_median, inplace=True)\n",
        "train_df[\"RoomService\"].fillna(room_service_median, inplace=True)\n",
        "train_df[\"FoodCourt\"].fillna(food_court_median, inplace=True)\n",
        "train_df[\"ShoppingMall\"].fillna(shopping_mall_median, inplace=True)\n",
        "train_df[\"Spa\"].fillna(spa_median, inplace=True)\n",
        "train_df[\"VRDeck\"].fillna(vr_deck_median, inplace=True)\n",
        "train_df[\"HomePlanet\"].fillna(home_planet_mode, inplace=True)\n",
        "train_df[\"Destination\"].fillna(destination_mode, inplace=True)\n",
        "train_df[\"VIP\"].fillna(vip_mode, inplace=True)\n",
        "train_df[\"CryoSleep\"].fillna(cryosleep_mode, inplace=True)\n",
        "train_df[\"Deck\"].fillna(deck_mode, inplace=True)\n",
        "train_df[\"Side\"].fillna(side_mode, inplace=True)\n",
        "train_df[\"Num\"].fillna(num_median, inplace=True)\n",
        "\n",
        "\n",
        "# Fill NA values in test_df\n",
        "test_df[\"Age\"].fillna(age_median, inplace=True)\n",
        "test_df[\"RoomService\"].fillna(room_service_median, inplace=True)\n",
        "test_df[\"FoodCourt\"].fillna(food_court_median, inplace=True)\n",
        "test_df[\"ShoppingMall\"].fillna(shopping_mall_median, inplace=True)\n",
        "test_df[\"Spa\"].fillna(spa_median, inplace=True)\n",
        "test_df[\"VRDeck\"].fillna(vr_deck_median, inplace=True)\n",
        "test_df[\"HomePlanet\"].fillna(home_planet_mode, inplace=True)\n",
        "test_df[\"Destination\"].fillna(destination_mode, inplace=True)\n",
        "test_df[\"VIP\"].fillna(vip_mode, inplace=True)\n",
        "test_df[\"CryoSleep\"].fillna(cryosleep_mode, inplace=True)\n",
        "test_df[\"Deck\"].fillna(deck_mode, inplace=True)\n",
        "test_df[\"Side\"].fillna(side_mode, inplace=True)\n",
        "test_df[\"Num\"].fillna(num_median, inplace=True)\n"
      ],
      "metadata": {
        "id": "Xl8W8ydjKLID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "uD3wBxd_OM6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.isnull().sum()"
      ],
      "metadata": {
        "id": "IcE1R9iVPeUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can drop the Cabin column\n",
        "train_df = train_df.drop('Cabin', axis=1)\n",
        "test_df = test_df.drop('Cabin', axis=1)"
      ],
      "metadata": {
        "id": "KZuKjKTOYFwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After all these steps we need to use One-Hot Encoding for our columns. Because, remember, ML models only understand numbers and we have categorical columns that are strings. We use **.get_dummies()** to turn those single categorical columns into multiple binary columns."
      ],
      "metadata": {
        "id": "qvBIAfdeAoQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hot encode test and train dataframes\n",
        "train_df = pd.get_dummies(train_df)\n",
        "test_df = pd.get_dummies(test_df)"
      ],
      "metadata": {
        "id": "86S2asWtYpjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to syncronized the test dataframe with the train dataframe so both dataframes' columns are in the same order to be able to use then."
      ],
      "metadata": {
        "id": "E8c8lUMxBTvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Synce columns between the train and test sets\n",
        "test_df = test_df.reindex(columns=train_df.columns, fill_value=0)"
      ],
      "metadata": {
        "id": "3XCT1X8rfv6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data"
      ],
      "metadata": {
        "id": "BNCVS45af0Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X\n",
        "X = train_df.drop(\"Transported\", axis = 1)\n",
        "# Create y\n",
        "y = train_df[\"Transported\"]"
      ],
      "metadata": {
        "id": "kH0vN4TmZ1gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "len(X_train), len(X_test), len(y_train), len(y_test)"
      ],
      "metadata": {
        "id": "SUUYqC7hhyQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to scale numerical columns to ensure model fairness and stability because we have columns with different scales. Doing this ensures all features contribute equally to the model."
      ],
      "metadata": {
        "id": "-pGFV2NfmrtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale numerical columns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "numerical_cols = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Num', 'Number_Group']\n",
        "scaler = StandardScaler()\n",
        "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
        "X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])"
      ],
      "metadata": {
        "id": "KZuJ0fJGiaAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Modelling"
      ],
      "metadata": {
        "id": "QXG2gFooajD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Put models in a dicctionary\n",
        "models = {\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}"
      ],
      "metadata": {
        "id": "7moWnMjhjPs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to fit and score models\n",
        "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fits and evaluates given machine learning models.\n",
        "    models : a dict of different Scikit-Learn machine learning models\n",
        "    X_train : training data (no labels)\n",
        "    X_test : testing data (no labels)\n",
        "    y_train : training labels\n",
        "    y_test : test labels\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Make dictinoary to keep model scores\n",
        "    model_scores = {}\n",
        "\n",
        "    # Loop through models\n",
        "    for name, model in models.items():\n",
        "        # Fit the model to the data\n",
        "        model.fit(X_train, y_train)\n",
        "        # Evaluate the model and append its score to model_scores\n",
        "        model_scores[name] = model.score(X_test, y_test)\n",
        "    return model_scores"
      ],
      "metadata": {
        "id": "BpGR1PyOjqgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results = fit_and_score(models, X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "P03KJxKrFkBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results"
      ],
      "metadata": {
        "id": "lzJBc0CWFkVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [200],\n",
        "    'max_depth': [3],\n",
        "    \"learning_rate\": [0.025, 0.05, 0.075, 0.1],\n",
        "    \"min_samples_leaf\": [20],\n",
        "    \"subsample\": [0.8]\n",
        "    }\n",
        "\n",
        "# Instantiate the Grid search object\n",
        "gscv = GridSearchCV(\n",
        "    estimator=GradientBoostingClassifier(),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    verbose=2,\n",
        "    scoring = \"accuracy\"\n",
        ")\n",
        "\n",
        "gscv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Bo4VaTCau_tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gscv.best_score_"
      ],
      "metadata": {
        "id": "EO0tVPd5HCMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gscv.best_params_"
      ],
      "metadata": {
        "id": "Oduqj_BW1DX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model with all information\n",
        "final_model = GradientBoostingClassifier(random_state=42, learning_rate=0.05, max_depth=3, n_estimators=200, min_samples_leaf=20, subsample=0.8)\n",
        "final_model.fit(X,y)"
      ],
      "metadata": {
        "id": "sNesO4JQGIWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ],
      "metadata": {
        "id": "YYzAUHLbGjgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the Transported column from test_df\n",
        "test_df = test_df.drop('Transported', axis=1)"
      ],
      "metadata": {
        "id": "-3AxOztFIYO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions = final_model.predict(test_df)\n",
        "final_predictions"
      ],
      "metadata": {
        "id": "vAUW-gMdHHQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create submission file"
      ],
      "metadata": {
        "id": "5Ls5d2RTIoK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/spaceship-titanic.zip"
      ],
      "metadata": {
        "id": "I9B19lYIIuGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import GradientBoostingClassifier # Assuming this was your best model\n",
        "\n",
        "# --- 1. Load the original test data to retrieve the PassengerId ---\n",
        "# We'll assume the file path is the same as before.\n",
        "original_test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Extract the PassengerId column before we use the prepared test_df\n",
        "passenger_ids = original_test_df['PassengerId']\n",
        "\n",
        "# --- 2. Final Model Training (Recap) ---\n",
        "# Assuming X and y were your final, prepared training features/target\n",
        "# And final_model was trained on X and y\n",
        "\n",
        "# --- 3. Final Prediction (Recap) ---\n",
        "# Assuming you already dropped the 'Transported' column from the prepared test_df\n",
        "# final_predictions = final_model.predict(test_df) # Run this line again if needed\n",
        "\n",
        "# --- 4. Create the Submission DataFrame ---\n",
        "submission_df = pd.DataFrame({\n",
        "    'PassengerId': passenger_ids,\n",
        "    'Transported': final_predictions\n",
        "})\n",
        "\n",
        "# Convert the predictions (which are 0/1 integers) to True/False booleans as required by Kaggle\n",
        "submission_df['Transported'] = submission_df['Transported'].astype(bool)\n",
        "\n",
        "# --- 5. Save the Submission File ---\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file 'submission.csv' successfully created!\")"
      ],
      "metadata": {
        "id": "31rVKCdRJMk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "04UkE2NnJHKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}