{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg-VFxKavaEN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGNM9fkb2TXs"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets huggingface_hub fsspec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69BzPQjxaJg0"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY8P2DiIwiBe"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN1E85UDxXr4"
      },
      "source": [
        "## 1. Getting a dataset\n",
        "\n",
        "Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
        "\n",
        "\n",
        "* `text`: a string feature.\n",
        "* `label`: a classification label, with possible values including `neg` (0), `pos` (1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Us1Zm6ixxeY5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"stanfordnlp/imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7zkuJLu25nP"
      },
      "outputs": [],
      "source": [
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otN6SuW026ds"
      },
      "outputs": [],
      "source": [
        "# What features are there?\n",
        "ds.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tvFq6DY3oWZ"
      },
      "outputs": [],
      "source": [
        "# Access the training split\n",
        "ds[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcjiIcn48har"
      },
      "outputs": [],
      "source": [
        "ds[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8zi0Bgl8nfz"
      },
      "source": [
        "### 1.1 Inspect random examples from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcopxTZV80ZD"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random_indx = random.sample(range(len(ds[\"train\"])),5)\n",
        "random_samples = ds[\"train\"][random_indx]\n",
        "\n",
        "print(f\"[INFO] Random samples from dataset:\\n\")\n",
        "for item in zip(random_samples[\"text\"], random_samples[\"label\"]):\n",
        "    print(f\"Text: {item[0]} | Label: {item[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwPa8qdd9QIu"
      },
      "outputs": [],
      "source": [
        "# Get unique label values\n",
        "ds[\"train\"].unique(\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ABtJBLA9fh7"
      },
      "outputs": [],
      "source": [
        "# Check number of each label\n",
        "from collections import Counter\n",
        "\n",
        "Counter(ds[\"train\"][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I-Lbj7L-eAN"
      },
      "source": [
        "## 2. Prepare data for text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdV5bLHI-9FU"
      },
      "source": [
        "### 2.1 Tokenize text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGRyFAN8A8ps"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = \"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FR-TsTsDh_j"
      },
      "outputs": [],
      "source": [
        "# Test the tokenizer\n",
        "tokenizer(\"I love pizza\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdhGXFrnEqHd"
      },
      "source": [
        "### 2.2 Making a preprocessing function to tokenize text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1EInsaNGB3k"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(examples):\n",
        "  \"\"\" Tokenize a given example text and return the tokenized text.\"\"\"\n",
        "  return tokenizer(examples[\"text\"], padding= \"max_length\", truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX5FNgykGYpv"
      },
      "outputs": [],
      "source": [
        "# Map our tokenize_text function to dataset\n",
        "tokenized_dataset = ds.map(function = tokenize_text,\n",
        "                           batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtL7c9lMHN0G"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjeSZJMAIFI9"
      },
      "outputs": [],
      "source": [
        "# Get two samples from the tokenized dataset\n",
        "train_tokenized_sample = tokenized_dataset[\"train\"][0]\n",
        "test_tokenized_sample = tokenized_dataset[\"test\"][0]\n",
        "\n",
        "for key in train_tokenized_sample.keys():\n",
        "    print(f\"[INFO] Key: {key}\")\n",
        "    print(f\"Train sample: {train_tokenized_sample[key]}\")\n",
        "    print(f\"Test sample: {test_tokenized_sample[key]}\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atmXXOTmZsqK"
      },
      "source": [
        "### Setup evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g0Bkz7UZv5S"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n",
        "  \"\"\"\n",
        "  Computes the accuracy of a model by comparing the predictions and labels.\n",
        "  \"\"\"\n",
        "  predictions, labels = predictions_and_labels\n",
        "\n",
        "  # Get highest prediction probability of each prediction if predictions are probabilities\n",
        "  if len(predictions.shape) >= 2:\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "  return accuracy_metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7fUguAPRcb6"
      },
      "source": [
        "## 3. Setting up a model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COd6kmWXSujC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Setup model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path = \"distilbert/distilbert-base-uncased\",\n",
        "    num_labels = 2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eynlwZ0VUiVZ"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gzJ_9I6UirY"
      },
      "source": [
        "### 3.1 Create a directory for saving models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1vMVFw4U3VA"
      },
      "outputs": [],
      "source": [
        "# Create model output directory\n",
        "from pathlib import Path\n",
        "\n",
        "# Create models directory\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok = True)\n",
        "\n",
        "# Create model save name\n",
        "model_save_name = \"text_clasification-imdb-distilbert-base-uncased\"\n",
        "\n",
        "# Create model save path\n",
        "model_save_dir = Path(models_dir, model_save_name)\n",
        "\n",
        "model_save_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ni3t5wJiDZ0P"
      },
      "source": [
        "### 3.2 Setting up traning arguments with TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ARly0XdLQ94"
      },
      "source": [
        "Parameters we are going to use for training our model:\n",
        " * `output_dir`: the output directory where the model predictions and checkpoints will be written.\n",
        " * `learning_rate`: value of the initial learning rate for AdamW optimizer\n",
        " * `per_device_train_batch_size`:the batch size per device for training\n",
        " * `per device_eval_batch_size`: the batch size per device for evaluation\n",
        " * `num_train_epochs`:total number of training epochs to perfomr\n",
        " * `eval_strategy`: the evaluation strategy to adopt during training. Posible values are:\n",
        "  * `\"no\"`: no evaluation is done during training\n",
        "  * `\"steps\"`: evlauation is done every `eval_steps`\n",
        "  * `\"epoch\"`: evaluation is done at the ed of each epoch\n",
        " * `save_strategy`: the checkpoing save stragtegy to adopt during training\n",
        " * `save_total_limit`: if a value is passed, will limit the total amout of checkpoints.\n",
        " * `use_cpu`:\n",
        " * `seed`:random seed for repdoducibility\n",
        " * `load_best_model_at_the_end`:whether or not to load the best model found during training.\n",
        " * `logging_strategy`:\n",
        " * `report_to`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzlPAYCDHfMO"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "print(f\"[INFO] Saving model checkpoints to: {model_save_dir}\")\n",
        "\n",
        "# Create training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = model_save_dir,\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size = 32,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    num_train_epochs = 5,\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    save_total_limit = 3,\n",
        "    seed = 42,\n",
        "    load_best_model_at_end = True,\n",
        "    logging_strategy = \"epoch\",\n",
        "    report_to = \"none\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zetKXNFWa7K"
      },
      "source": [
        "### 3.3 Setting up an instance of Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GHOJ4CIWhXP"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Setup Trainer\n",
        "trainer = Trainer(\n",
        "   model = model,\n",
        "   args = training_args,\n",
        "   train_dataset = tokenized_dataset[\"train\"],\n",
        "   eval_dataset = tokenized_dataset[\"test\"],\n",
        "   processing_class = tokenizer,\n",
        "   compute_metrics = compute_accuracy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYnn7PlqaEPV"
      },
      "source": [
        "### 3.4 Training our text classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NXUVw8IjanDd"
      },
      "outputs": [],
      "source": [
        "# Train a text classification model\n",
        "results = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjaD4PkfatjZ"
      },
      "source": [
        "### 3.5 Save the model for later use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH_oRFb8Cjf0"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "print(f\"[INFO] Saving model to {model_save_dir}\")\n",
        "trainer.save_model(output_dir = model_save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtHnLKCzCwoY"
      },
      "source": [
        "### 3.6 Inspecting the model training metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5txMJ9k3C1C-"
      },
      "outputs": [],
      "source": [
        "trainer_history_all = trainer.state.log_history\n",
        "trainer_history_metrics = trainer_history_all[:-1] # get everything except the training time metrics\n",
        "\n",
        "trainer_history_metrics[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81RLMR0DDXnk"
      },
      "outputs": [],
      "source": [
        "import pprint # import pretty print for nice printing of lists\n",
        "\n",
        "# Extract training and evaluation metrics\n",
        "trainer_history_training_set = []\n",
        "trainer_history_eval_set = []\n",
        "\n",
        "# Loop through metrics and filter for training and eval metrics\n",
        "for item in trainer_history_metrics:\n",
        "    item_keys = list(item.keys())\n",
        "    # Check to see if \"eval\" is in the keys of the item\n",
        "    if any(\"eval\" in item for item in item_keys):\n",
        "        trainer_history_eval_set.append(item)\n",
        "    else:\n",
        "        trainer_history_training_set.append(item)\n",
        "\n",
        "# Show the first two items in each metric set\n",
        "print(f\"[INFO] First two items in training set:\")\n",
        "pprint.pprint(trainer_history_training_set[:2])\n",
        "\n",
        "print(f\"\\n[INFO] First two items in evaluation set:\")\n",
        "pprint.pprint(trainer_history_eval_set[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6nk6qTKhpUp"
      },
      "outputs": [],
      "source": [
        "# Create pandas DataFrames for the training and evaluation metrics\n",
        "trainer_history_training_df = pd.DataFrame(trainer_history_training_set)\n",
        "trainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n",
        "\n",
        "trainer_history_training_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghEOH4KehsDy"
      },
      "outputs": [],
      "source": [
        "trainer_history_eval_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQqo9OxuhvJq"
      },
      "outputs": [],
      "source": [
        "# Plot training and evaluation loss\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(trainer_history_training_df[\"epoch\"], trainer_history_training_df[\"loss\"], label=\"Training loss\")\n",
        "plt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Evaluation loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Text classification with DistilBert training and evaluation loss over time\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU-AIRARhzq3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}