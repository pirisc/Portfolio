{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Titanic\n",
        "\n",
        "The main goal os the competition is to predict which passengers survides the Titanic shipwreck.\n",
        "\n",
        "* Link to the competition: https://www.kaggle.com/competitions/titanic"
      ],
      "metadata": {
        "id": "zwvokgjzd-Wt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data\n",
        "\n",
        "We had two different files:\n",
        "* train.csv\n",
        "* test.csv\n",
        "\n",
        "We will need to predic the value of `Survived` feature on the test dataset."
      ],
      "metadata": {
        "id": "zTFQsYY1eRSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle"
      ],
      "metadata": {
        "id": "hDpXsUZ_eoYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "15v0Cg1-fLL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve credentials\n",
        "KAGGLE_KEY =  userdata.get('KAGGLE_KEY')\n",
        "KAGGLE_USERNAME = userdata.get('KAGGLE_USERNAME')\n",
        "\n",
        "# Set environmental variables with %env to better work with kaggle\n",
        "%env KAGGLE_USERNAME=$KAGGLE_USERNAME\n",
        "%env KAGGLE_KEY=$KAGGLE_KEY"
      ],
      "metadata": {
        "id": "4-6DFYu8fQ_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c titanic"
      ],
      "metadata": {
        "id": "oUANc8VsfSCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/titanic.zip"
      ],
      "metadata": {
        "id": "ISGaW3oSfXbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect Data"
      ],
      "metadata": {
        "id": "t07fbyCyfbEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_ids = test_df[\"PassengerId\"].copy()"
      ],
      "metadata": {
        "id": "lTAehDR_gkeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the train_df\n",
        "train_df"
      ],
      "metadata": {
        "id": "4pYYuTGggpSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "id": "oab8P_qogsqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "MPSLwKE1hDbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check how many data is missing\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "CijXQUS6hHNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.isnull().sum()"
      ],
      "metadata": {
        "id": "tO-JvoVDKNB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data"
      ],
      "metadata": {
        "id": "LlUD2IvxhOvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate median for Age column\n",
        "age_median = train_df[\"Age\"].median()\n",
        "# Calculate Fare for test_df\n",
        "fare_median = train_df[\"Fare\"].median()\n",
        "# Calculate mode for Embarked column\n",
        "embarked_mode = train_df[\"Embarked\"].mode()[0]\n",
        "\n",
        "# Fill NA values\n",
        "train_df[\"Age\"].fillna(age_median, inplace=True)\n",
        "test_df[\"Age\"].fillna(age_median, inplace=True)\n",
        "\n",
        "test_df[\"Fare\"].fillna(fare_median, inplace=True)\n",
        "\n",
        "train_df[\"Embarked\"].fillna(embarked_mode, inplace=True)"
      ],
      "metadata": {
        "id": "DAR2y0JBiYnm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column called Has_cabin\n",
        "train_df[\"Has_cabin\"] = train_df[\"Cabin\"].notna().astype(int)\n",
        "test_df[\"Has_cabin\"] = test_df[\"Cabin\"].notna().astype(int)\n",
        "\n",
        "# Deck: first letter or M for missing\n",
        "train_df[\"Deck\"] = train_df[\"Cabin\"].str[0].fillna(\"M\") # M = missing\n",
        "test_df[\"Deck\"] = test_df[\"Cabin\"].str[0].fillna(\"M\")"
      ],
      "metadata": {
        "id": "yjnGh2Tjr3BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Title from Name\n",
        "def extract_title(name):\n",
        "  if pd.isna(name):\n",
        "    return \"Unknown\"\n",
        "  try:\n",
        "    # Last, Title. First\n",
        "    return name.split(\",\")[1].split(\".\")[0].strip()\n",
        "  except:\n",
        "    return \"Unknown\"\n",
        "\n",
        "train_df[\"Title\"] = train_df[\"Name\"].apply(extract_title)\n",
        "test_df[\"Title\"] = test_df[\"Name\"].apply(extract_title)"
      ],
      "metadata": {
        "id": "XNucN51-s-QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create rare titles column\n",
        "title_counts = train_df['Title'].value_counts()\n",
        "rare_titles = set(title_counts[title_counts < 10].index)\n",
        "train_df['Title'] = train_df['Title'].apply(lambda t: 'Rare' if t in rare_titles else t)\n",
        "test_df['Title']  = test_df['Title'].apply(lambda t: 'Rare' if t in rare_titles else t)\n"
      ],
      "metadata": {
        "id": "XJ-fdinMt_U9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Title'].value_counts()"
      ],
      "metadata": {
        "id": "PCdX44qnxgRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Family features\n",
        "train_df[\"Family_Size\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\n",
        "test_df[\"Family_Size\"] = test_df[\"SibSp\"] + test_df[\"Parch\"] + 1\n",
        "\n",
        "# Create Family group based on Family size\n",
        "train_df[\"Family_Group\"] = train_df[\"Family_Size\"].apply(lambda x: \"Alone\" if x ==1  else (\"Small\" if x <5 else \"Big\"))\n",
        "test_df[\"Family_Group\"] = test_df[\"Family_Size\"].apply(lambda x: \"Alone\" if x ==1  else (\"Small\" if x <5 else \"Big\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "MxZaI7eO2sg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns we don't want\n",
        "drop_train_cols = ['Name', 'Ticket', 'Cabin']   # we keep PassengerId in train only if you want debugging; usually drop before fit\n",
        "drop_test_cols  = ['Name', 'Ticket', 'Cabin']\n",
        "\n",
        "train_df = train_df.drop(columns=[c for c in drop_train_cols if c in train_df.columns])\n",
        "test_df  = test_df.drop(columns=[c for c in drop_test_cols  if c in test_df.columns])\n"
      ],
      "metadata": {
        "id": "ZIn0cjI45-ln",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concat and get_dumines\n",
        "n_train = len(train_df)\n",
        "# Make a copy of Survived to reattach later\n",
        "y = train_df['Survived'].astype(int)\n",
        "\n",
        "# concat train WITHOUT Survived and test\n",
        "combined = pd.concat([train_df.drop(columns=['Survived']), test_df], axis=0, sort=False)\n",
        "\n",
        "# One-hot encode. We already filled Deck missing with 'M' so no dummy_na needed.\n",
        "combined = pd.get_dummies(combined, drop_first=True)\n",
        "\n",
        "# Split back\n",
        "train_proc = combined.iloc[:n_train].copy()\n",
        "test_proc  = combined.iloc[n_train:].copy()\n",
        "\n",
        "# Reattach Survived to train_proc\n",
        "train_proc['Survived'] = y.values\n",
        "\n",
        "# Safety: ensure test has same columns as train (excluding Survived)\n",
        "test_proc = test_proc.reindex(columns=[c for c in train_proc.columns if c != 'Survived'], fill_value=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "0gR61hEB7ErP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data"
      ],
      "metadata": {
        "id": "ML2HMfbFiBqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8) Now create feature matrix X and labels y (and keep test_X for final preds)\n",
        "# -------------------------\n",
        "X_full = train_proc.drop(columns=['Survived'])\n",
        "y_full = train_proc['Survived']\n",
        "\n",
        "X_test_for_submission = test_proc.copy()"
      ],
      "metadata": {
        "id": "X7YSs_c8uNNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, stratify=y_full, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "ndBNYL9XjHYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10) Scale numeric columns FITTING THE SCALER ON X_train ONLY\n",
        "# -------------------------\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "numerical_cols = ['Age', 'Fare']\n",
        "\n",
        "# Check numeric columns exist (if not, warn)\n",
        "for c in numerical_cols:\n",
        "    if c not in X_train.columns:\n",
        "        raise KeyError(f\"Numeric column {c} not present in training features: {X_train.columns.tolist()[:20]}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train.loc[:, numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
        "X_val.loc[:, numerical_cols]   = scaler.transform(X_val[numerical_cols])\n",
        "X_test_for_submission.loc[:, numerical_cols] = scaler.transform(X_test_for_submission[numerical_cols])\n"
      ],
      "metadata": {
        "id": "GMCg9hPTuRm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Modelling"
      ],
      "metadata": {
        "id": "KUC5AwXxjhEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Put models in a dicctionary\n",
        "models = {\n",
        "    \"KNN\": KNeighborsClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}"
      ],
      "metadata": {
        "id": "AWZPNdhkkHtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to fit and score models\n",
        "def fit_and_score(models, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"\n",
        "    Fits and evaluates given machine learning models.\n",
        "    models : a dict of different Scikit-Learn machine learning models\n",
        "    X_train : training data (no labels)\n",
        "    X_test : testing data (no labels)\n",
        "    y_train : training labels\n",
        "    y_test : test labels\n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Make dictinoary to keep model scores\n",
        "    model_scores = {}\n",
        "\n",
        "    # Loop through models\n",
        "    for name, model in models.items():\n",
        "        # Fit the model to the data\n",
        "        model.fit(X_train, y_train)\n",
        "        # Evaluate the model and append its score to model_scores\n",
        "        model_scores[name] = model.score(X_test, y_test)\n",
        "    return model_scores"
      ],
      "metadata": {
        "id": "WUCCUXrCkI8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "Pd4XwgJyEeQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results = fit_and_score(models, X_train, X_val, y_train, y_val)"
      ],
      "metadata": {
        "id": "5MBjSWAfkJ_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_results"
      ],
      "metadata": {
        "id": "beifuOk7kMXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the grid of parameters to search\n",
        "param_grid = {\n",
        "    \"penalty\": [\"l1\", \"l2\"],\n",
        "    \"C\": [0.01, 0.1, 1, 10],\n",
        "    \"solver\": [\"saga\"],\n",
        "    \"max_iter\": [200, 500]\n",
        "}\n",
        "\n",
        "# Instantiate the Grid search object\n",
        "gscv = GridSearchCV(\n",
        "    estimator=LogisticRegression(),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    scoring = \"accuracy\"\n",
        ")\n",
        "\n",
        "gscv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "BrYOjxntkNBN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model with this information\n",
        "best_params = gscv.best_params_\n",
        "final_model = LogisticRegression(**{k: v for k, v in best_params.items() if k in LogisticRegression().get_params()})\n",
        "final_model.random_state = 42\n",
        "final_model.max_iter = max(final_model.get_params().get('max_iter', 100), 300)\n",
        "final_model.fit(X_full, y_full)\n"
      ],
      "metadata": {
        "id": "f7aVErGik8Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ],
      "metadata": {
        "id": "ua2yHInElRHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 14) Prepare test set for prediction and create submission\n",
        "# -------------------------\n",
        "# test set is already aligned (X_test_for_submission) and scaled earlier\n",
        "# ensure no Survived or PassengerId in features\n",
        "if 'Survived' in X_test_for_submission.columns:\n",
        "    X_test_for_submission = X_test_for_submission.drop(columns=['Survived'])\n",
        "\n",
        "# Predict labels (Titanic usually expects 0/1 labels)\n",
        "final_preds = final_model.predict(X_test_for_submission)"
      ],
      "metadata": {
        "id": "_BZ_EYKdvC7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a submission file"
      ],
      "metadata": {
        "id": "uH3CIfd1lbxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    \"PassengerId\": test_ids,\n",
        "    \"Survived\": final_preds.astype(int)\n",
        "})\n",
        "\n",
        "# Save to csv\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission saved to submission.csv\")\n"
      ],
      "metadata": {
        "id": "sCyizWg_wERR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcTnqIKAv_j5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
