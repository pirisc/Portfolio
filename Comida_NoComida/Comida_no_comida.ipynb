{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv_yVQLT-150"
      },
      "source": [
        "# Comida/No Comida Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_rwnIiZ_npL"
      },
      "source": [
        "## 1. Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sOKgQTBAhIZ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install dependecies (this is mostly for Google Colab)\n",
        "try:\n",
        "  import datasets, evaluate, accelerate\n",
        "  import gradio as gr\n",
        "except ModuleNotFoundError:\n",
        "  !pip install -U datasets evaluate accelerate gradio\n",
        "  import datasets, evaluate, accelerate\n",
        "  import gradio as gr\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pq4Fao5Bk32"
      },
      "outputs": [],
      "source": [
        "print(f\"Using transfomers version: {transformers.__version__}\")\n",
        "print(f\"Using torch version: {torch.__version__}\")\n",
        "print(f\"Using datasets version: {datasets.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtKWgXW4DEP1"
      },
      "source": [
        "## 2. Getting a dataset\n",
        "\n",
        "We are using the dataset [\"comida_no_comida\"](https://huggingface.co/datasets/Sairii/comida_no_comida) from Hugging Face.\n",
        "\n",
        "It consists in ~1000 text phrases labeled `comida` and `no_comida`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hdu3fUctDGJb"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset= load_dataset(\"Sairii/comida_no_comida\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOid7rqjH_8s"
      },
      "outputs": [],
      "source": [
        "# What features are there?\n",
        "dataset.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXqDmgueI7Qw"
      },
      "outputs": [],
      "source": [
        "# Access the training split\n",
        "dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UNy1J9tJJNt"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TA0btf6JOrJ"
      },
      "source": [
        "### Inspect random samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iifnF5dJbVn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random_indexes = random.sample(range(len(dataset[\"train\"])),5)\n",
        "print(random_indexes)\n",
        "\n",
        "random_samples = dataset[\"train\"][random_indexes]\n",
        "random_samples\n",
        "\n",
        "print(f\"[INFO] Random samples from dataset:\\n\")\n",
        "for text, label in zip(random_samples[\"text\"], random_samples[\"label\"]):\n",
        "  print(f\"Text: {text} | Label: {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5DwgoJaJk2A"
      },
      "outputs": [],
      "source": [
        "# Get unique label values\n",
        "dataset[\"train\"].unique(\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0Ftv9mwKmDF"
      },
      "outputs": [],
      "source": [
        "# Check the count of each label\n",
        "from collections import Counter\n",
        "\n",
        "Counter(dataset[\"train\"][\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVt-hupqLCcD"
      },
      "outputs": [],
      "source": [
        "# Turn our dataset into a Dataframe and get a random sample\n",
        "food_not_food_df = pd.DataFrame(dataset[\"train\"])\n",
        "food_not_food_df.sample(7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyDRUXepLS6F"
      },
      "outputs": [],
      "source": [
        "food_not_food_df[\"label\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKx0EtT3LaT4"
      },
      "source": [
        "## 3. Preparing data for text classification\n",
        "\n",
        "We want to:\n",
        "1. Tokenize our text - turn our text into numbers (this goes for labels as well).\n",
        "2. Create a train/test split - want to train our model on the training split and want to evaluate our model on the test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7CmugzRMh6i"
      },
      "outputs": [],
      "source": [
        "# Create a mapping for labels to numeric value\n",
        "id2label = {0: \"no_comida\", 1:\"comida\"}\n",
        "label2id = {\"no_comida\": 0, \"comida\": 1}\n",
        "\n",
        "print(id2label)\n",
        "print(label2id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDEAfO0RXQzY"
      },
      "outputs": [],
      "source": [
        "# Create mappings programmatically from dataset\n",
        "id2label = {idx: label for idx, label in enumerate(dataset[\"train\"].unique(\"label\")[::-1])} # reverse sort list to have \"not_food\" first\n",
        "label2id = {label: idx for idx, label in id2label.items()}\n",
        "\n",
        "print(f\"Label to ID mapping: {label2id}\")\n",
        "print(f\"ID to Label mapping: {id2label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZblMnPwhTKXD"
      },
      "outputs": [],
      "source": [
        "# Turn labels into 0 or 1\n",
        "def map_labels_to_number(example):\n",
        "  example[\"label\"] = label2id[example[\"label\"]]\n",
        "  return example\n",
        "\n",
        "\n",
        "example_sample = {\"text\": \"Esta frase es sobre mi comida favorita, la miel \", \"label\":\"comida\"}\n",
        "\n",
        "# Test our ucntion\n",
        "map_labels_to_number(example_sample)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rnM6lU7U1G1"
      },
      "outputs": [],
      "source": [
        "# Map our dataset labels to numbers (the whole thing)\n",
        "dataset = dataset[\"train\"].map(map_labels_to_number)\n",
        "dataset[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEb141wcXXUZ"
      },
      "outputs": [],
      "source": [
        "# Shuffle data and look at 5 more random samples\n",
        "dataset.shuffle()[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QJOMa-pX91I"
      },
      "source": [
        "### Split the dataset into training and test sets\n",
        "\n",
        "* Train set = model will learn patterns on this dataset\n",
        "* Validation set (optional) = we can tune our model's hyperparameters on this set\n",
        "* Test set = model will evaluate patternes on this dataset\n",
        "\n",
        "We can split our dataset using `datasets.Dataset.train_test_split`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNXmulK15hVD"
      },
      "outputs": [],
      "source": [
        "# Split our dataset into train/test splits\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op4P4r4G7SnJ"
      },
      "outputs": [],
      "source": [
        "random_idx_train = random.randint(0, len(dataset[\"train\"]))\n",
        "random_sample_train = dataset[\"train\"][random_idx_train]\n",
        "random_sample_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSngPy0D78XL"
      },
      "outputs": [],
      "source": [
        "random_idx_test = random.randint(0, len(dataset[\"test\"]))\n",
        "random_sample_test = dataset[\"test\"][random_idx_test]\n",
        "random_sample_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Yt6WJw8Ofd"
      },
      "source": [
        "### Tokenizing our text data (turning text into numbers)\n",
        "\n",
        "The premise of tokenization is to turn words into numbers.\n",
        "\n",
        ">E.g. \"I love pizza!\" -> [30, 145, 678, 999]\n",
        "\n",
        "\n",
        "The `transformers` library has in-built support for Hugging Face `tokenizers`.\n",
        "\n",
        "And the calls `transformers.AutoTokenizer` helps pair a model to a tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Coi8p2bT_YU7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n",
        "                                          use_fast=True)\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR_tA_3qHzji"
      },
      "outputs": [],
      "source": [
        "# Test out tokenizer\n",
        "tokenizer(\"Me gustan las magdalenas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS666ZaJPJKk"
      },
      "source": [
        "* input_ids = our text turned into numbers\n",
        "* attention_mask = wheter or not to pay attention to certain tokens (1 = yes, pay attention; 0 = no, don't pay attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7l5upKEO4nh"
      },
      "outputs": [],
      "source": [
        "# Try adding an \"!\" at the end\n",
        "tokenizer(\"Me gustan las magdalenas!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aScYFNgDPA-F"
      },
      "outputs": [],
      "source": [
        "# Get the length of the vocabulary\n",
        "len_tokenizer_vocab = len(tokenizer.vocab)\n",
        "len_tokenizer_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWo-DHqOPoxM"
      },
      "outputs": [],
      "source": [
        "# Max sequence lenght\n",
        "max_tokenizer_sequence_len = tokenizer.model_max_length\n",
        "max_tokenizer_sequence_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cRamGljRXS2"
      },
      "outputs": [],
      "source": [
        "tokenizer.vocab[\"iris\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NShL0yx7Rcey"
      },
      "source": [
        "### Making a preprocessing function to tokenize text\n",
        "\n",
        "Want to make it easy to go from sample -> tokenized_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auzWyQ4Ep34N"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(examples):\n",
        "  \"\"\"\n",
        "  Tokenize given example text and return tokenized text.\n",
        "  \"\"\"\n",
        "  return tokenizer(examples[\"text\"],\n",
        "                   padding=True, # pad short sequences to longest len in batch (if sample len=100, sample will be paddes to 512 or longest sample in batch )\n",
        "                   truncation=True) # truncate long sequences in the max len the moel can handle (e.g. if sample len =1000, model len = 512, sample will be shortened to 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIO3Mwk1rHeC"
      },
      "outputs": [],
      "source": [
        "example_sample_2 = {\"text\": \"Me gusta la pizza\", \"label\":1}\n",
        "\n",
        "# Test de function\n",
        "tokenize_text(example_sample_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X0KuvpLrLdU"
      },
      "outputs": [],
      "source": [
        "long_text = \"Me gusta la pizza\" * 1000\n",
        "len(long_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUpZhl6Hsk3A"
      },
      "outputs": [],
      "source": [
        "tokenized_long_text = tokenize_text({\"text\": long_text, \"label\":1})\n",
        "len(tokenized_long_text[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o00iVlojsuKO"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zswy4B3GtUsb"
      },
      "outputs": [],
      "source": [
        "# Map our tokenize text function to the dataset\n",
        "tokenized_dataset = dataset.map(function=tokenize_text,\n",
        "                                batched=True, #=True to tokenize across batches of samples at a time rather than one at a time\n",
        "                                batch_size=1000)\n",
        "\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDS2OEKOttW6"
      },
      "source": [
        "> 游댐 **Note:** in ML, it is often faster to do things in batches rather than one at a time due to leveraging computer hardware parallelization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3psXdO62uAeQ"
      },
      "outputs": [],
      "source": [
        "# Get two samples from the tokenized datasets\n",
        "train_tokenized_sample = tokenized_dataset[\"train\"][0]\n",
        "test_tokenized_sample = tokenized_dataset[\"test\"][0]\n",
        "\n",
        "for key in train_tokenized_sample.keys():\n",
        "  print(f\"[INFO] Key: {key}\")\n",
        "  print(f\"Train sample: {train_tokenized_sample[key]}\")\n",
        "  print(f\"Test sample: {test_tokenized_sample[key]}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdFqrnsQuuNn"
      },
      "source": [
        "### Tokenization takeaways\n",
        "\n",
        "1. Tokenizers = turn data into numbers (e.g. text -> map to number)\n",
        "2. Many models are out there and have differente tokenizers, Hugging Face's `Auto` (e.g. `AutoTokenizer`, `AutoProcessor`, `AutoModel` etc help to match tokenizers to models)\n",
        "3. Tokenization can happen in parallel using `map` and batched funtions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvRiY4OxwDX7"
      },
      "source": [
        "## 4. Setting up an evaluation metric\n",
        "\n",
        "What we want to do: use the evaluation metric to get a numerical idea of how our model is performing.\n",
        "\n",
        "Some commom evaluation metrics for classification:\n",
        "\n",
        "- Accuracy (how many examples out of 100, did you get correct?\n",
        "- Precision\n",
        "- Recall\n",
        "- F1 Score\n",
        "\n",
        "Evaluation metric is important because some project may have an evaluation threshold you need to fulfill."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgqm3g80qijn"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_accuracy(predictions_and_labels: Tuple[np.array, np.array]):\n",
        "  \"\"\"\n",
        "  Computes the accuracy of a model by comparing the predictions and labels\n",
        "  \"\"\"\n",
        "  predictions, labels = predictions_and_labels\n",
        "\n",
        "  if len(predictions.shape) >= 2:\n",
        "    predictions = np.argmax(predictions, axis =1)\n",
        "\n",
        "  return accuracy_metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miq0uHargTVD"
      },
      "outputs": [],
      "source": [
        "# Example predictions and accuracy score\n",
        "example_preds_all_correct = np.array([0,0,0,0,0,0,0,0,0,0])\n",
        "example_preds_one_incorrect = np.array([0,0,0,0,1,0,0,0,0,0])\n",
        "example_labels =np.array([0,0,0,0,0,0,0,0,0,0])\n",
        "\n",
        "# Test the function\n",
        "print(f\"Accuracy when all predictions are correct: {compute_accuracy((example_preds_all_correct, example_labels ))}\")\n",
        "print(f\"Accuracy when one prediction is incorrect: {compute_accuracy((example_preds_one_incorrect, example_labels ))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8GtzeXOhDJO"
      },
      "source": [
        "## 5. Setting up a model for training\n",
        "\n",
        "We're going to be using transfer learning.\n",
        "\n",
        "Transfer learnign is a powerful technique, unique to deep learning models that enables us to use the patterns one model has learned on another problem for our own problem\n",
        "\n",
        "Workflow for training:\n",
        "\n",
        "1. Create and preprocess data\n",
        "2. Define the model we want to use for our problem: https://huggingface.co/models\n",
        "3. Define training arguments for training our model `tranformers.TrainingArguments`\n",
        " * These are also know as \"hyperparameters\" = settings of your model that you can adjust.\n",
        " * Parameters = weights/patterns in the model that get updated automatically.\n",
        "4. Pass `TrainingArguments` to an instance of `tranformers.Trainer`\n",
        "5. Train the model by calling `Trainer.train()`\n",
        "6. Save the model (to our local machine or to Hugging Face Hub)\n",
        "7. Evaluate the trained model by making and inspecting predictions on the test data (and our own custom data)\n",
        "8. Turn the model into a shareable demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLwBY6ZHhmlq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    pretrained_model_name_or_path=\"distilbert/distilbert-base-uncased\",\n",
        "    num_labels = 2,\n",
        "    id2label = id2label,\n",
        "    label2id = label2id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjB95mIkAeSz"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WUMjphyBLVA"
      },
      "source": [
        "Our model is comprised of the following part:\n",
        "1. `embeddings` - embeddings are a form of learned representation of tokens. So if tokens are a direct mapping from token to number, embeddings are a learned vector representation.\n",
        "2. `transformer` - our model architecture backbone, this has discovered patterns/relationships in the embeddings.\n",
        "3. `classifier` - we need to customize this layer to suit our problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PV99YxLDT2w"
      },
      "source": [
        "> **Note:** If you get input errors from passing a sample to a model, make sure the sample you pass to your model is formatting in the same way your model was trained on. For example if your model used a specific tokenizer, make sure to tokenize your text before passing it to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCOeqf5FBEMK"
      },
      "source": [
        "### Count the parameters in our model\n",
        "\n",
        "Weights/parameters = small numeric opportunites for a model to learn patterns in data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7Sr3DoNE-YI"
      },
      "outputs": [],
      "source": [
        "def count_params(model):\n",
        "  \"\"\"\n",
        "  Count the parameters of a PyTorch model\n",
        "  \"\"\"\n",
        "  trainable_parameters = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
        "  total_parameters = sum(param.numel() for param in model.parameters())\n",
        "\n",
        "  return {\"trainable_parameters\":trainable_parameters,\n",
        "          \"total_parameters\": total_parameters}\n",
        "\n",
        "count_params(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwbmDrm_MeaD"
      },
      "source": [
        "Looks like our model has around 67M parameters and **all** of them are trainable.\n",
        "\n",
        "Note:\n",
        "* Generally, the more parameters a model has to learn.\n",
        "* For comparison models such as Llama 3 8B has 8 billion parameters.\n",
        "* If you want the best possible performance, generally more parameters is better.\n",
        "  * However, with more parameters requiers more compute + time\n",
        "  * You'll be surprised how well a smaller model can perfomr with specific data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc6_zvmlLRit"
      },
      "outputs": [],
      "source": [
        "# Create model output directory\n",
        "from pathlib import Path\n",
        "\n",
        "# Create models dir\n",
        "models_dir = Path(\"models\")\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Create model save name\n",
        "model_save_name = \"comida_no_comida_text_classifier\"\n",
        "\n",
        "# Create model save path\n",
        "model_save_dir = Path(models_dir, model_save_name)\n",
        "\n",
        "model_save_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nLOzHVm5JV_"
      },
      "source": [
        "### Setting up training arguments (hyperparameters) with TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4BLSWfNAzkI"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "print(f\"[INFO] Saving model checkpoint: {model_save_dir}\")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = model_save_dir,\n",
        "    learning_rate = 0.0001,\n",
        "    per_device_train_batch_size = BATCH_SIZE,\n",
        "    per_device_eval_batch_size = BATCH_SIZE,\n",
        "    num_train_epochs = 10,\n",
        "    eval_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    save_total_limit = 3,\n",
        "    use_cpu = False,\n",
        "    seed = 42,\n",
        "    load_best_model_at_end = True,\n",
        "    logging_strategy = \"epoch\",\n",
        "    report_to = \"none\"\n",
        "    # push_to_hub = True # if you want your model to save to the hugging face after training\n",
        "    # hub_private_repo = False # repo public o private? (default: public)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXTrLDwUnwnc"
      },
      "source": [
        "### Setting up an instance of Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3yMB4awoUk1"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Setup Trainer instance\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_dataset[\"train\"],\n",
        "    eval_dataset = tokenized_dataset[\"test\"],\n",
        "    tokenizer = tokenizer,\n",
        "    compute_metrics = compute_accuracy\n",
        ")\n",
        "\n",
        "trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBKoc39dq__2"
      },
      "source": [
        "### Train the model by calling `Trainer.train()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZWjB9U48pfDD"
      },
      "outputs": [],
      "source": [
        "results = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zQqRLX6LtQw8"
      },
      "outputs": [],
      "source": [
        "# Inspect training metrics\n",
        "for key, value in results.metrics.items():\n",
        "  print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZtLZEUTwaDM"
      },
      "source": [
        "### Save the model for later use\n",
        "\n",
        "> **Note:** If you are saving a model to Google Colab, note that it will dissapear from your Colab instance when it disconnects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pQXCNB6oyQqH"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "print(f\"[INFO] Saving model to {model_save_dir}\")\n",
        "trainer.save_model(output_dir=model_save_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1h4HAWzyYeX"
      },
      "source": [
        "### Inspect the model training metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mSvpAxpW8Ibg"
      },
      "outputs": [],
      "source": [
        "# Get training history\n",
        "# Get training history\n",
        "trainer_history_all = trainer.state.log_history\n",
        "trainer_history_metrics = trainer_history_all[:-1] # get everything except the training time metrics (we've seen these already)\n",
        "trainer_history_training_time = trainer_history_all[-1] # this is the same value as results.metrics from above\n",
        "\n",
        "# View the first 4 metrics from the training history\n",
        "trainer_history_metrics[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OpNssByT9A46"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "# Extract eval and training metrics\n",
        "trainer_history_training_set = []\n",
        "trainer_history_eval_set = []\n",
        "\n",
        "# Loop through our metrics\n",
        "for item in trainer_history_metrics:\n",
        "  item_keys = list(item.keys())\n",
        "  if any(\"eval\" in item for item in item_keys):\n",
        "    trainer_history_eval_set.append(item)\n",
        "  else:\n",
        "    trainer_history_training_set.append(item)\n",
        "\n",
        "# Show the first item from eahc\n",
        "print(f\"First two items in training set:\")\n",
        "pprint.pprint(trainer_history_training_set[:2])\n",
        "\n",
        "print(f\"\\nFirst to items in eval epochs:\")\n",
        "pprint.pprint(trainer_history_eval_set[:2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlTSbFZ09qsH"
      },
      "source": [
        "### Plot loss cruves\n",
        "\n",
        "Loss curves = a good visualiaztion of your model's performance over time.\n",
        "\n",
        "Ideally your loss curves will trend downwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Uhxrh441_UDQ"
      },
      "outputs": [],
      "source": [
        "# Create a pandas DataFrame for the training and evaluation metrics\n",
        "trainer_history_train_df = pd.DataFrame(trainer_history_training_set)\n",
        "trainer_history_eval_df = pd.DataFrame(trainer_history_eval_set)\n",
        "\n",
        "trainer_history_train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R9nwYJN3_4W1"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(trainer_history_train_df[\"epoch\"], trainer_history_train_df[\"loss\"], label = \"Training loss\")\n",
        "plt.plot(trainer_history_eval_df[\"epoch\"], trainer_history_eval_df[\"eval_loss\"], label=\"Eval loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Text classification fine-tuning DistilBert training and evaluation loss over time\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niZhax8fAXW2"
      },
      "source": [
        "### Pushing our model to the Hugging Face Hub\n",
        "\n",
        "Why do this?\n",
        "\n",
        "So we can share our model.\n",
        "\n",
        "Other people can try it out.\n",
        "\n",
        "We can keep history of different model versions\n",
        "\n",
        "To save to the Hugging Face Hub, we can use the `Trainer.push_to_hub` method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Klcxqrv4Exnu"
      },
      "outputs": [],
      "source": [
        "# Save our model to the Hugging Face Hub\n",
        "model_upload_url = trainer.push_to_hub(\n",
        "    commit_message =  \"Sbudo text classifier\"\n",
        ")\n",
        "\n",
        "print(f\"Model successfully uploaded to the Hugging Face Hub with URL: {model_upload_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqfyUK8yFx1j"
      },
      "source": [
        "### Making and evaluating predictions on the test data\n",
        "\n",
        "Evaluating a model is just as important as training a model.\n",
        "\n",
        "We can make predictions on the test data using the `Trainer.predict`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lg2ZyoMn45U0"
      },
      "outputs": [],
      "source": [
        "# Perfom predictions on the test data\n",
        "predictions_all = trainer.predict(tokenized_dataset[\"test\"])\n",
        "prediction_values = predictions_all.predictions\n",
        "predictions_metrics = predictions_all.metrics\n",
        "\n",
        "print(f\"[INFO] Prediction metrics on the test data:\")\n",
        "predictions_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGuVJJl3AVs6"
      },
      "source": [
        "> **Note:** If you want a good evaluation method, make predictions on your entire test dataset, then index on the prediction which are wrong but have high prediction probabilty.\n",
        "\n",
        "For example, get the top 100-1000 and go through all of the examples where the model's prediction had high probabilitye but was incorrect -> this often lead to great insights into your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGLxkoat6NLC"
      },
      "source": [
        "### Let's get prediction probabilities and evaluate by hand"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XknzKEnc7Dxk"
      },
      "outputs": [],
      "source": [
        "# Predicted logits (raw output of the model) -> prediction probabilities with torch.softmax -> predicted labels\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Get predictions probabilites with torch.softmax\n",
        "pred_probs = torch.softmax(torch.tensor(prediction_values), dim=1)\n",
        "\n",
        "# 2. Get predicted labels\n",
        "pred_labels = torch.argmax(pred_probs, axis=1)\n",
        "\n",
        "# 3. Get the true labels\n",
        "true_labels = tokenized_dataset[\"test\"][\"label\"]\n",
        "\n",
        "# 4. Compute prediction labels to true labels and get the test accuracy\n",
        "test_accuracy = accuracy_score(y_true= true_labels,\n",
        "                              y_pred = pred_labels)\n",
        "\n",
        "# 3. Get the true labels\n",
        "true_labels = tokenized_dataset[\"test\"][\"label\"]\n",
        "\n",
        "# 4. Compute prediction labels to true labels and get the test accuracy\n",
        "test_accuracy = accuracy_score(y_true= true_labels,\n",
        "                              y_pred = pred_labels)\n",
        "\n",
        "print(f\"[INFO] Test accuracy: {test_accuracy*100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBqbo55M_Gqw"
      },
      "source": [
        "### Exploring our models prediction probabilites\n",
        "\n",
        "It's a very good way to evaluate a model by sorting prediction probabilites and seeing where the model went wrong."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dTRy809ZCjQ9"
      },
      "outputs": [],
      "source": [
        "# Make a dataframe of test predictions\n",
        "test_predictions_df= pd.DataFrame({\n",
        "    \"text\": dataset[\"test\"][\"text\"],\n",
        "    \"true_label\" : true_labels,\n",
        "    \"pred_label\" : pred_labels,\n",
        "    \"pred_prob\": torch.max(pred_probs, dim=1).values\n",
        "})\n",
        "\n",
        "test_predictions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JGCMr8H4DWEm"
      },
      "outputs": [],
      "source": [
        "# Show 10 examples with low prediction probability\n",
        "test_predictions_df.sort_values(\"pred_prob\", ascending=True).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "564f_HZ4D92Z"
      },
      "source": [
        "## 6. Making and inspecting predictions on custom text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynkABYmLGf2Q"
      },
      "source": [
        "### Discussing ways to make predictions (inference)\n",
        "\n",
        "* Note: Whenever you hear the word \"inference\" it means to use a model to make predictions on data.\n",
        "\n",
        "Two main ways to perfom inference:\n",
        "\n",
        "1. **Pipeline mode** - Using `transfomers.pipeline` to load our model and perform text classification.\n",
        "\n",
        "2. **PyTorch mode** - Using a combination of `transformers.AutoTokenizer` and `transformers.AutoModelForSequenceClassification` and passing each our target model name.\n",
        "\n",
        "Each mode supports:\n",
        "\n",
        "1. Predictions one at a time (fast but can be slower with many many samples).\n",
        " * Helpul for a say a comment system and comments happen spordicaly, to predict wheter the comment was \"spam\" or \"not spam\".\n",
        "2. Batches or predictions at a time (faster but up to a point).\n",
        " * Helpful for when you have a large static database or many samples coming in at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "StMhuoLQOufi"
      },
      "outputs": [],
      "source": [
        "# Setup our device for making predictions\n",
        "# Note: generally the faster the hardware acceleator, the faster the predictions\n",
        "# For example if you have a dedicated GPU, you should use it over CPU\n",
        "\n",
        "def set_device():\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  elif torch.backends.mps.is_available() and torch.backneds.mps.is_built():\n",
        "    device = torch.device(\"mps\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  return device\n",
        "\n",
        "DEVICE = set_device()\n",
        "print(f\"[INFO] Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "80aF0QLjG31I"
      },
      "outputs": [],
      "source": [
        "local_model_path = \"/content/models/comida_no_comida_text_classifier\"\n",
        "\n",
        "huggingface_model_path=\"Sairii/comida_no_comida_text_classifier\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxfszZlAGrxU"
      },
      "source": [
        "### Making predictions with pipeline mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ceZvcEuI1pw_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# create an instance of transformers.pipeline\n",
        "food_not_food_classifier = pipeline(task=\"text-classification\",\n",
        "                                    model = local_model_path,\n",
        "                                    device = DEVICE,\n",
        "                                    top_k = 1,\n",
        "                                    batch_size = BATCH_SIZE)\n",
        "\n",
        "food_not_food_classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "66ZwmYdF4qlO"
      },
      "outputs": [],
      "source": [
        "test_custom_sentence = \"Un plato de frutas\"\n",
        "food_not_food_classifier(test_custom_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kFRUc_sU4z9L"
      },
      "outputs": [],
      "source": [
        "test_no_food_sentence = \"Un coche del 2020\"\n",
        "food_not_food_classifier(test_no_food_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MT1nON_W5VTk"
      },
      "outputs": [],
      "source": [
        "# Use pipeline with a model from Hugging face\n",
        "food_not_food_classifier = pipeline(task = \"text-classification\",\n",
        "                                    model = huggingface_model_path,\n",
        "                                    device=DEVICE,\n",
        "                                    top_k=1,\n",
        "                                    batch_size=BATCH_SIZE)\n",
        "\n",
        "food_not_food_classifier(test_no_food_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVDrkZTS-FJb"
      },
      "source": [
        "### Making multiple predictions at the same time with batch prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UAG52RT2_i0W"
      },
      "outputs": [],
      "source": [
        "# Create a list of sentences to make predictions on\n",
        "sentences = [\n",
        "    \"Las estrellas en el cielo nos recuerdan lo peque침os que somos en el universo.\",\n",
        "    \"Un buen libro puede transportarte a mundos que nunca imaginaste.\",\n",
        "    \"El sonido de la lluvia golpeando la ventana es una melod칤a para el alma.\",\n",
        "    \"Nada como la brisa del mar para sentirte libre por un momento.\",\n",
        "    \"Las palabras tienen el poder de sanar o destruir, elige bien las tuyas.\",\n",
        "    \"Los abrazos sinceros son el mejor refugio en los d칤as dif칤ciles.\",\n",
        "    \"La m칰sica tiene la capacidad de revivir recuerdos con solo unas notas.\",\n",
        "    \"El tiempo es el recurso m치s valioso que tenemos, 칰salo sabiamente.\",\n",
        "    \"Las monta침as nos ense침an que lo mejor siempre est치 en la cima.\",\n",
        "    \"La creatividad es el motor que impulsa el cambio en el mundo.\"\n",
        "\n",
        "]\n",
        "\n",
        "food_not_food_classifier(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-vh_ZL88AD0S"
      },
      "outputs": [],
      "source": [
        "food_captions = [\n",
        "    \"Nada como el aroma del pan reci칠n horneado en la ma침ana.\",\n",
        "    \"Un buen plato de pasta puede arreglar cualquier d칤a.\",\n",
        "    \"El chocolate derretido es la definici칩n de felicidad en estado l칤quido.\",\n",
        "    \"Las frutas frescas son como peque침os regalos de la naturaleza.\",\n",
        "    \"No hay mejor compa침칤a para una pel칤cula que unas palomitas de ma칤z crujientes.\",\n",
        "    \"El queso fundido mejora cualquier comida, sin excepci칩n.\",\n",
        "    \"Una taza de caf칠 caliente es el mejor comienzo para cualquier jornada.\",\n",
        "    \"El sushi es una obra de arte hecha de arroz y pescado.\",\n",
        "    \"Las especias pueden transformar un plato simple en una experiencia inolvidable.\",\n",
        "    \"Un buen postre es el final perfecto para cualquier comida.\"\n",
        "]\n",
        "\n",
        "not_food_captions = [\n",
        "     \"El sonido de las olas rompiendo en la orilla es pura paz.\",\n",
        "    \"Un buen libro es una puerta a infinitas aventuras.\",\n",
        "    \"Las estrellas nos recuerdan lo vasto que es el universo.\",\n",
        "    \"La m칰sica tiene el poder de transportarnos a otro tiempo y lugar.\",\n",
        "    \"Nada como una caminata bajo la lluvia para aclarar la mente.\",\n",
        "    \"Los abrazos son la mejor medicina para el alma.\",\n",
        "    \"El arte es la forma m치s pura de expresar lo que sentimos.\",\n",
        "    \"Cada amanecer nos da una nueva oportunidad para empezar de nuevo.\",\n",
        "    \"La fotograf칤a nos permite capturar momentos que nunca volver치n.\",\n",
        "    \"El tiempo es el recurso m치s valioso que tenemos, 칰salo sabiamente.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "THBu2095A7Sj"
      },
      "outputs": [],
      "source": [
        "food_not_food_classifier(food_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mNig07AEA-m8"
      },
      "outputs": [],
      "source": [
        "\n",
        "food_not_food_classifier(not_food_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LZDHrk5-BBAB"
      },
      "outputs": [],
      "source": [
        "confusing_captions = [\n",
        "   \"El sonido de las olas rompiendo en la orilla es pura paz.\",\n",
        "    \"Un buen libro es una puerta a infinitas aventuras.\",\n",
        "    \"Las estrellas nos recuerdan lo vasto que es el universo.\",\n",
        "    \"La m칰sica tiene el poder de transportarnos a otro tiempo y lugar.\",\n",
        "    \"Nada como una caminata bajo la lluvia para aclarar la mente.\",\n",
        "    \"Los abrazos son la mejor medicina para el alma.\",\n",
        "    \"El arte es la forma m치s pura de expresar lo que sentimos.\",\n",
        "    \"Cada amanecer nos da una nueva oportunidad para empezar de nuevo.\",\n",
        "    \"La fotograf칤a nos permite capturar momentos que nunca volver치n.\",\n",
        "    \"El tiempo es el recurso m치s valioso que tenemos, 칰salo sabiamente.\"\n",
        "]\n",
        "\n",
        "food_not_food_classifier(confusing_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fch9IGpCBVMB"
      },
      "source": [
        "### Time our model accros larger sample sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ogYqc_3iC06a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Create 1000 sentences\n",
        "sentences_1000 = sentences * 100\n",
        "\n",
        "# Time how long it takes to make predictions on all sentences (one at a time)\n",
        "print(f\"[INFO] Number of sentences: {len(sentences_1000)}\")\n",
        "start_time_one_at_a_time = time.time()\n",
        "for sentence in sentences_1000:\n",
        "  # Make a prediction\n",
        "  food_not_food_classifier(sentence)\n",
        "end_time_one_at_a_time  = time.time()\n",
        "\n",
        "total_time_one_at_a_time = end_time_one_at_a_time  - start_time_one_at_a_time\n",
        "avg_time_per_pred = total_time_one_at_a_time / len(sentences_1000)\n",
        "print(f\"[INFO] Total time for making predicions on {len(sentences_1000)}: {total_time_one_at_a_time}s\")\n",
        "\n",
        "print(f\"[INFO] Avg time per prediction:{avg_time_per_pred}s \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7UaCjJWsDzCa"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Let's now use piplein in batches\n",
        "for i in [10, 100, 1000, 10000]:\n",
        "  sentences_big = sentences * i\n",
        "  print(f\"[INFO] Number of sentences: {len(sentences_big)}\")\n",
        "\n",
        "  start_time = time.time()\n",
        "  # Predict on all setences in batch model\n",
        "  food_not_food_classifier(sentences_big)\n",
        "  end_time = time.time()\n",
        "\n",
        "  total_time_per_all_sentences = end_time - start_time\n",
        "  avg_time_per_sentence = total_time_per_all_sentences / len(sentences_big)\n",
        "  print(f\"[INFO] Inferece time for {len(sentences_big)} sentences: {round(total_time_one_at_a_time,6)}s\")\n",
        "  print(f\"[INFO] Avg inference time per sentence: {round(avg_time_per_sentence,8)}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgLopiw6FqbR"
      },
      "source": [
        "### Making predictions with PyTorch\n",
        "\n",
        "Steps with PyTorch predictions:\n",
        "\n",
        "1. Create the tokenizer with `AutoTokenizer`.\n",
        "2. Create the model with `AutoModel` (`AutoModelForSequenceClassification`)\n",
        "3. Tokenize text with 1\n",
        "4. Make predictions with 2\n",
        "5. Format predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vl1nfRO-NNyy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Setup the model path\n",
        "model_path = \"Sairii/comida_no_comida_text_classifier\"\n",
        "\n",
        "# Create an example to predict on\n",
        "sample_food_text = \"Un delicioso plato de huevos, con beicon y patatas\"\n",
        "# Prepare the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_path)\n",
        "inputs = tokenizer(sample_food_text,\n",
        "                   return_tensors = \"pt\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvGNwQEgNyaF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load our text classification model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trpRBf38VuqG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  outputs = model(**inputs) #\"**\" means input all of the diccitionary keys as named arguments\n",
        "  outputs_verbose = model(input_ids = inputs[\"input_ids\"],\n",
        "                          attention_mask = inputs[\"attention_mask\"])\n",
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb7Yu2kTYCmY"
      },
      "outputs": [],
      "source": [
        "outputs_verbose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NwJYiFxYO_t"
      },
      "outputs": [],
      "source": [
        "# Convert logits to prediction probability  + labels\n",
        "output_logits = outputs.logits\n",
        "predicted_class_id = outputs.logits.argmax().item()\n",
        "predicted_class_label = model.config.id2label[predicted_class_id]\n",
        "predicted_probability = torch.softmax(outputs.logits,dim = 1).max().item()\n",
        "\n",
        "print(f\"Text: {food_captions}\")\n",
        "print(f\"Predicted class: {predicted_class_label} (prob: {predicted_probability * 100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h5Y4AencHyZ"
      },
      "source": [
        "## 7. Turning our model into a demo\n",
        "\n",
        "Turning a model into a demo helps you share it with others so they can try it out.\n",
        "\n",
        "And can potentially shaed some insights into how our model could be improved.\n",
        "\n",
        "We're going to create a machine learning model demo with Gradio: https://www.gradio.app/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md_ALdJWDss4"
      },
      "source": [
        "### Creating a simple function to perform inference\n",
        "\n",
        "1. Take an input of string\n",
        "2. Setup a text classification pipeline\n",
        "3. Get the output from the piplein\n",
        "4. Return the output from the pipleine in stpe 3 as formatted dictionary with the format: `{\"label_1\": probability_1, \"label_2\": probability_2}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNMjJv1NFTAL"
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "\n",
        "# 1. Create a function to take a string input\n",
        "def food_not_food_classifier(text: str) -> Dict[str, float]:\n",
        "  # 2. Setup food not food text classifier\n",
        "  food_not_food_classifier_pipeline= pipeline(task=\"text-classification\",\n",
        "                                     model=local_model_path,\n",
        "                                     batch_size = 32,\n",
        "                                     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                     top_k = None) # return all possible labels\n",
        "\n",
        "  # 3. GEt the outputs from our pipeline\n",
        "  outputs = food_not_food_classifier_pipeline(text)[0]\n",
        "\n",
        "  # 4. Format ouput for Gradio\n",
        "  output_dict = {}\n",
        "  for item in outputs:\n",
        "    output_dict[item[\"label\"]] = item[\"score\"]\n",
        "\n",
        "  return output_dict\n",
        "\n",
        "\n",
        "food_not_food_classifier(text = \"You are building a demo!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI0tU5XVQPcl"
      },
      "source": [
        "### Build a small Gradio demo to run locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3Li2Px4R2F9"
      },
      "outputs": [],
      "source": [
        "# 1. Import gradio\n",
        "import gradio as gr\n",
        "\n",
        "# 2. Create a gradio interface https://www.gradio.app/main/docs/gradio/interface#demos\n",
        "demo = gr.Interface(\n",
        "    fn = food_not_food_classifier,\n",
        "    inputs = \"text\",\n",
        "    outputs = gr.Label(num_top_classes=2),\n",
        "    title=\"Comida No Comida \",\n",
        "    description = \"Un classificador de texto que te indica si la frase es sobre comida o no\",\n",
        "    examples =[[\"Me gusta ver la nieve caer por la ventana\"],\n",
        "               [\"Un plato de huevos con patatas\"]]\n",
        "\n",
        ")\n",
        "\n",
        "# 3. Launch the interface\n",
        "demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeVV3zYXTsF8"
      },
      "source": [
        "## 8. Making our demo publicly accessible\n",
        "\n",
        "There are two main ways to make our demo accesibl with Hugging Face Spaces:\n",
        "\n",
        "1. Manually - We can go to huggingface.co/spaces -> \"Create new space\" -> add our files and publish!\n",
        "\n",
        "2. Programmatically - We can use the Hugging Face Hub Python API and add our files to a Space with code.\n",
        "\n",
        "To create a Space programmatically we're going to create three files:\n",
        "\n",
        "1. `app.py` - This is the main app with funtionality of our demo.\n",
        "2. `requirements.txt` - These are the dependecie which our app will require\n",
        "3. `README.md` - This will explain what our project/demo is about. And will also add some metada in YAML format.\n",
        "\n",
        "To create these we'll use the following file structure:\n",
        "```\n",
        "demos/\n",
        "較덕較 food_not_food_text_classifier/\n",
        "    較럭較 app.py\n",
        "    較럭較 README.md\n",
        "    較덕較 requirements.txt\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHZG03aEU1Py"
      },
      "source": [
        "### Making a directory to store our demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abk_KcsCXAaB"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Make directory for demos\n",
        "demos_dir = Path(\"../demos\")\n",
        "demos_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Create a folder for the food_not_foot_text_classifier demo\n",
        "food_not_food_text_classifier_demo_dir = Path(demos_dir, \"food_not_food_text_classifier\")\n",
        "food_not_food_text_classifier_demo_dir.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVaf4wCjcHNe"
      },
      "source": [
        "### Making an app.py file\n",
        "\n",
        "Our `app.py` will contain the main logic of our application to run\n",
        "\n",
        "When we upload it to Hugging Face Spaces, Spaces will try to run `app.py` automatically.\n",
        "\n",
        "In our `app.py` file we want to:\n",
        "\n",
        "1. Import packages.\n",
        "2. Define our function to use our model (this will work with Gradio)\n",
        "3. Create a demo with Gradio\n",
        "4. Run the demo with demo.launch()\n",
        "\n",
        "To create each of or files, we're going to use the magic command %%writefile\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kSKc-XlcUAg"
      },
      "outputs": [],
      "source": [
        "%%writefile ../demos/food_not_food_text_classifier/app.py\n",
        "# 1.Import the requiere files\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "from typing import Dict\n",
        "from transformers import pipeline\n",
        "\n",
        "# 2. Define our function to use with our model\n",
        "def food_not_food_classifier(text: str) -> Dict[str, float]:\n",
        "  # 2. Setup food not food text classifier\n",
        "  food_not_food_classifier_pipeline= pipeline(task=\"text-classification\",\n",
        "                                     model=\"Sairii/comida_no_comida_text_classifier\",\n",
        "                                     batch_size = 32,\n",
        "                                     device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "                                     top_k = None) # return all possible labels\n",
        "\n",
        "  # 3. GEt the outputs from our pipeline\n",
        "  outputs = food_not_food_classifier_pipeline(text)[0]\n",
        "\n",
        "  # 4. Format ouput for Gradio\n",
        "  output_dict = {}\n",
        "  for item in outputs:\n",
        "    output_dict[item[\"label\"]] = item[\"score\"]\n",
        "\n",
        "  return output_dict\n",
        "\n",
        "# 3. Create a gradio interface\n",
        "description = \"\"\"\n",
        "Un classificador de texto que te indica si la frase es sobre comida o no.\n",
        "\"\"\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn = food_not_food_classifier,\n",
        "    inputs = \"text\",\n",
        "    outputs = gr.Label(num_top_classes=2),\n",
        "    title = \"游볮游냆游꼢游뛂 Clasificador de Comida o No Comida\",\n",
        "    description = description,\n",
        "    examples = [[\"Me gusta ver la nieve caer por la ventana\"],\n",
        "               [\"Un plato de huevos con patatas\"]]\n",
        "\n",
        ")\n",
        "\n",
        "# 4.Launch the interfeace\n",
        "if __name__ == \"__main__\":\n",
        "  demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y6p0keMSlrK"
      },
      "source": [
        "游볮游냆游꼢游뛂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaSy3IzCWRd-"
      },
      "source": [
        "### Making a README file\n",
        "\n",
        "This file is in markdown format.\n",
        "\n",
        "With a special YAML block at the top.\n",
        "\n",
        "The YAML block at the top is used for metada + settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kize2roNxoQ"
      },
      "outputs": [],
      "source": [
        "%%writefile ../demos/food_not_food_text_classifier/README.md\n",
        "---\n",
        "title: Clasificador de Texto sobre Comida o No Comida\n",
        "emoji: 游볮游냆游꼢游뛂\n",
        "colorFrom: blue\n",
        "colorTo: yellow\n",
        "sdk: gradio\n",
        "app_file: app.py\n",
        "pinned: false\n",
        "licence: apache-2.0\n",
        "---\n",
        "\n",
        "# 游볮游냆游꼢游뛂 Clasificador de Comida o No Comida\n",
        "\n",
        "\n",
        "Peque침a demo para ver un clasificador de texto que determina si una frase es sobre comida o no.\n",
        "\n",
        "DistilBERT model fine-tuned en un dataset con [995 capturas sobre comida/no_comida](https://huggingface.co/datasets/Sairii/comida_no_comida)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Z3C8MAb30X"
      },
      "source": [
        "### Making a requirements file\n",
        "\n",
        "This file is going to tell our Hugging Face Space wich version/which packages to use.\n",
        "\n",
        "If we don't create this file, we may get an error such as:\n",
        "\n",
        "===== Application Startup at 2024-06-13 05:37:21 =====\n",
        "\n",
        "Traceback (most recent call last):\n",
        "  File \"/home/user/app/app.py\", line 1, in <module>\n",
        "    import torch\n",
        "ModuleNotFoundError: No module named 'torch'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5NQG5YvaFHp"
      },
      "outputs": [],
      "source": [
        "%%writefile ../demos/food_not_food_text_classifier/requirements.txt\n",
        "gradio\n",
        "torch\n",
        "transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWFIPuEKc32H"
      },
      "source": [
        "### Uploading our demo to Hugging Face Spaces\n",
        "\n",
        "To do so, we'll use the Hugging FAce Hub Python API - https://huggingface.co/docs/huggingface_hub/en/package_reference/hf_api\n",
        "\n",
        "To get our demo hosted on Hugging Face Spaces we뗣l go through the following steps:\n",
        "\n",
        "1. Import the required methods from the huggingface_hub package, including `create_repo`, `get_full_repo_name`, `upload_file` (optional, we뗣l be using `upload_folder`) and upload_folder.\n",
        "2. Define the demo folder we뗛 like to upload as well as the different parameters for the Hugging Face Space such as repo type (\"space\"), our target Space name, the target Space SDK (\"gradio\"), our Hugging Face token with write access (optional if it already isn뗪 setup).\n",
        "3. Create a repository on Hugging Face Spaces using the `huggingface_hub.create_repo` method and filling out the appropriate parameters.\n",
        "4. Get the full name of our created repository using the `huggingface_hub.get_full_repo_name` method (we could hard code this but I like to get it programmatically incase things change).\n",
        "5. Upload the contents of our target demo folder (`../demos/food_not_food_text_classifier/`) to Hugging Face Hub with `huggingface_hub.upload_folder`.\n",
        "6. Hope it all works and inspect the results! 游"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c6AtH5QhgC5"
      },
      "outputs": [],
      "source": [
        "# 1. Import the required methods for uploading to the HF Hub\n",
        "from huggingface_hub import(\n",
        "    create_repo,\n",
        "    get_full_repo_name,\n",
        "    upload_file,\n",
        "    upload_folder\n",
        ")\n",
        "\n",
        "# 2. Define the parameters we'd like to use for uploading our Space\n",
        "LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD =\"../demos/food_not_food_text_classifier\"\n",
        "HF_TARGET_SPACE_NAME = \"learn_hf_food_not_food_text_classifier_demo\"\n",
        "HF_REPO_TYPE = \"space\"\n",
        "HF_SPACE_SDK=\"gradio\"\n",
        "\n",
        "# 3. Create a Space repo on Hugging Face Hub\n",
        "print(f\"[INFO] Creating repo on Hugging Face Hub with name: {HF_TARGET_SPACE_NAME}\")\n",
        "create_repo(\n",
        "    repo_id=HF_TARGET_SPACE_NAME,\n",
        "    repo_type = HF_REPO_TYPE,\n",
        "    private=False,\n",
        "    space_sdk = HF_SPACE_SDK,\n",
        "    exist_ok = True\n",
        "\n",
        ")\n",
        "\n",
        "# 4. Get the full repo name(e.g. {username}/{repo_name})\n",
        "hf_full_repo_name = get_full_repo_name(model_id = HF_TARGET_SPACE_NAME)\n",
        "print(f\"[INFO] Full Hugging Face Hub repo name: {hf_full_repo_name}\")\n",
        "\n",
        "# 5.Upload our demo folder\n",
        "print(f\"[INFO] Uploading {LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD} to repo:{hf_full_repo_name}\")\n",
        "folder_upload_url = upload_folder(\n",
        "    repo_id = hf_full_repo_name,\n",
        "    folder_path = LOCAL_DEMO_FOLDER_PATH_TO_UPLOAD,\n",
        "    path_in_repo = \".\",\n",
        "    repo_type=HF_REPO_TYPE,\n",
        "    commit_message = \"Uploading our foot not food text classifier demos from  a notebook\"\n",
        "\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Demo folder successfully with commit URL: {folder_upload_url} \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8MfhFokkxVR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
